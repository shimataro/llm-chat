{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc976d5-fe84-4020-89f1-b94d64c83d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8137966-fd23-4ab5-a83c-07bf0134c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMクラス\n",
    "# ./libs/llm.py と同じ\n",
    "import threading\n",
    "from typing import Any, Generator, Optional\n",
    "\n",
    "import torch\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
    "from transformers.models.auto.modeling_auto import AutoModelForSeq2SeqLM\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\",\n",
    "        access_token: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\" LLMの初期化\n",
    "\n",
    "        :param model_name: モデル名\n",
    "        :param access_token: Hugging Faceのアクセストークン\n",
    "        \"\"\"\n",
    "        self._model_name = model_name\n",
    "\n",
    "        # トークナイザーとモデルを読み込み\n",
    "        self._model = _load_model(model_name, access_token)\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=True,\n",
    "            token=access_token,\n",
    "        )\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        do_sample: bool = True,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\" 推論\n",
    "\n",
    "        :param input_text: 入力テキスト\n",
    "        :param max_new_tokens: 生成する最大トークン数\n",
    "        :param do_sample: 推論結果をサンプリングするか？Falseなら常に確率の一番高いトークンのみを出力する（結果は決定的になる）\n",
    "        :param temperature: サンプリングの多様性を制御する温度パラメーター\n",
    "        :param top_p: nucleus samplingの確率閾値\n",
    "        :return: トークンのジェネレーター\n",
    "\n",
    "        Examples:\n",
    "            >>> for token in llm.infer(\"こんにちは。\"):\n",
    "            ...     print(token, end=\"\", flush=True)\n",
    "        \"\"\"\n",
    "        tokenizer = self._tokenizer\n",
    "        model = self._model\n",
    "\n",
    "        # プロンプト生成→トークン生成→GPUメモリーに転送\n",
    "        prompt = self._prompt(input_text)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # 出力取得用のストリーマー\n",
    "        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "        thread = threading.Thread(\n",
    "            target=model.generate,\n",
    "            kwargs={\n",
    "                **inputs,\n",
    "                \"streamer\": streamer,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": do_sample,\n",
    "            }\n",
    "        )\n",
    "        thread.start()\n",
    "\n",
    "        for token in streamer:\n",
    "            yield token\n",
    "\n",
    "    def print_inference_result(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        do_sample: bool = True,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> None:\n",
    "        \"\"\" 推論結果を出力\n",
    "\n",
    "        :param input_text: 入力テキスト\n",
    "        :param max_new_tokens: 生成する最大トークン数\n",
    "        :param do_sample: 推論結果をサンプリングするか？Falseなら常に確率の一番高いトークンのみを出力する（結果は決定的になる）\n",
    "        :param temperature: サンプリングの多様性を制御する温度パラメーター\n",
    "        :param top_p: nucleus samplingの確率閾値\n",
    "        \"\"\"\n",
    "        # 入力プロンプト\n",
    "        for token in self.infer(\n",
    "            input_text,\n",
    "            max_new_tokens,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_p,\n",
    "        ):\n",
    "            # トークンを1つずつ出力\n",
    "            print(token, end=\"\", flush=True)\n",
    "\n",
    "        # 出力の終端で改行\n",
    "        print()\n",
    "\n",
    "    def _prompt(self, input_text: str) -> str:\n",
    "        \"\"\" 入力文字列から、モデルに合わせたプロンプトを生成する\n",
    "\n",
    "        :param input_text: 入力テキスト\n",
    "        :return: プロンプト\n",
    "        \"\"\"\n",
    "\n",
    "        # seq2seqはプロンプト用の加工不要\n",
    "        if isinstance(self._model, AutoModelForSeq2SeqLM):\n",
    "            return input_text\n",
    "\n",
    "        # chat_template に対応していれば使う（Mistral, Gemmaなど）\n",
    "        if hasattr(self._tokenizer, \"chat_template\"):\n",
    "            try:\n",
    "                return self._tokenizer.apply_chat_template(\n",
    "                    [\n",
    "                        {\"role\": \"user\", \"content\": input_text},\n",
    "                    ],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "            except Exception:\n",
    "                # fallback\n",
    "                pass\n",
    "\n",
    "        # モデルに合わせたプロンプトを生成\n",
    "        model_name = self._model_name.lower()\n",
    "        if \"elyza\" in model_name:\n",
    "            return f\"[INST] {input_text} [/INST]\"\n",
    "\n",
    "        if \"rinna\" in model_name:\n",
    "            return (\n",
    "                f\"ユーザー: {input_text}\\n\"\n",
    "                \"システム: \"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            \"llama-2\" in model_name or \"llama2\" in model_name or\n",
    "            \"mistral\" in model_name\n",
    "        ):\n",
    "            return f\"<s>[INST] {input_text} [/INST]</s>\"\n",
    "\n",
    "        if \"openchat\" in model_name:\n",
    "            return f\"<|user|>\\n{input_text}\\n<|assistant|>\\n\"\n",
    "\n",
    "        if \"llama-3\" in model_name or \"llama3\" in model_name:\n",
    "            return (\n",
    "                \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "                f\"{input_text}\\n\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "            )\n",
    "\n",
    "        # デフォルト（そのまま）\n",
    "        return input_text\n",
    "\n",
    "\n",
    "def _load_model(model_name: str, access_token: Optional[str]) -> Any:\n",
    "    \"\"\" モデルを読み込む\n",
    "\n",
    "    :param model_name: モデル名\n",
    "    :param access_token: Hugging Faceのアクセストークン\n",
    "    :return: モデル\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    if hasattr(config, \"is_encoder_decoder\") and config.is_encoder_decoder:\n",
    "        return AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=_dtype(),\n",
    "            token=access_token,\n",
    "        )\n",
    "    else:\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=_dtype(),\n",
    "            token=access_token,\n",
    "        )\n",
    "\n",
    "\n",
    "def _dtype() -> torch.dtype | str:\n",
    "    \"\"\" データ型を取得\n",
    "\n",
    "    :return: データ型\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # CUDAが有効ならfloat16を使う（\"auto\"のままだとV100環境でfloat32を選ぶことがある）\n",
    "        return torch.float16\n",
    "\n",
    "    return \"auto\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93f486-88ce-467d-82a5-21d86758f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャットアプリケーション\n",
    "# LLMクラスのインポートがないこととmain()の呼び出しがないこと以外は ./llm-chat.py と同じ\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    \"\"\" アプリケーションパラメーター \"\"\"\n",
    "    model_name: str\n",
    "    access_token: Optional[str]\n",
    "\n",
    "\n",
    "def parse_args(args: Optional[list[str]]) -> Parameters:\n",
    "    \"\"\" 引数を解析\n",
    "\n",
    "    :return: 解析結果\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"LLM Chat\")\n",
    "    parser.add_argument(\n",
    "        \"--model-name\", \"-m\",\n",
    "        help=\"使用するモデル名\",\n",
    "        default=\"elyza/ELYZA-japanese-Llama-2-7b-instruct\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--access-token\", \"-t\",\n",
    "        help=\"Hugging Faceのアクセストークン\",\n",
    "    )\n",
    "\n",
    "    # 引数を解析＆Parametersクラスに変換\n",
    "    ns = parser.parse_args(args=args)\n",
    "    return Parameters(**vars(ns))\n",
    "\n",
    "\n",
    "def main(argv: Optional[list[str]] = None) -> None:\n",
    "    \"\"\" メイン関数\n",
    "\n",
    "    :param args: 解析済み引数\n",
    "    :return: 終了ステータス\n",
    "    \"\"\"\n",
    "    # コマンドライン引数を取り出す\n",
    "    params = parse_args(argv)\n",
    "    print(f\"Model Name: {params.model_name}\")\n",
    "    print(f\"Access Token: {params.access_token}\")\n",
    "    print()\n",
    "\n",
    "    # モデルの初期化\n",
    "    llm = LLM(params.model_name, params.access_token)\n",
    "\n",
    "    print()\n",
    "    print(\"Now, let's talk!\")\n",
    "    print(\"Type 'exit' to end the conversation.\")\n",
    "    print()\n",
    "\n",
    "    # ひたすらチャット\n",
    "    while True:\n",
    "        try:\n",
    "            # 入力プロンプト\n",
    "            print(\"> \", end=\"\", flush=True)\n",
    "\n",
    "            input_text = input().strip()\n",
    "            if input_text == \"exit\":\n",
    "                break\n",
    "\n",
    "            llm.print_inference_result(input_text)\n",
    "\n",
    "            # 次の入力プロンプトとの間隔を空ける\n",
    "            print()\n",
    "\n",
    "        except EOFError:\n",
    "            # EOFでも終了\n",
    "            break\n",
    "\n",
    "    print(\"See you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルやトークンを指定する場合は、コマンドライン引数のように渡す\n",
    "# 例) main([\"-m\", \"MODEL_NAME\", \"-t\", \"YOUR_ACCESS_TOKEN\"])\n",
    "main([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
