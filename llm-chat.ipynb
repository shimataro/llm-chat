{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc976d5-fe84-4020-89f1-b94d64c83d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8137966-fd23-4ab5-a83c-07bf0134c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMクラス\n",
    "# ./llm/llm.py と同じ\n",
    "from typing import Generator, Optional\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\",\n",
    "        access_token: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\" LLMの初期化\n",
    "\n",
    "        :param model_name: モデル名\n",
    "        \"\"\"\n",
    "        self._model_name = model_name\n",
    "\n",
    "        dtype: str | torch.dtype = \"auto\"\n",
    "        if torch.cuda.is_available():\n",
    "            # CUDAが有効ならfloat16を使う（\"auto\"のままだとV100環境でfloat32を選ぶことがある）\n",
    "            dtype = torch.float16\n",
    "\n",
    "        # トークナイザーとモデルを読み込み\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=dtype,\n",
    "            token=access_token,\n",
    "        )\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=True,\n",
    "            token=access_token,\n",
    "        )\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        do_sample: bool = True,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\" 推論\n",
    "\n",
    "        :param input_text: 入力テキスト\n",
    "        :param max_new_tokens: 生成する最大トークン数\n",
    "        :param do_sample: 推論結果をサンプリングするか？Falseなら常に確率の一番高いトークンのみを出力する（結果は決定的になる）\n",
    "        :param temperature: サンプリングの多様性を制御する温度パラメーター\n",
    "        :param top_p: nucleus samplingの確率閾値\n",
    "        :return: トークンのジェネレーター\n",
    "\n",
    "        Examples:\n",
    "            >>> for token in llm.infer(\"こんにちは。\"):\n",
    "            ...     print(token, end=\"\", flush=True)\n",
    "        \"\"\"\n",
    "        tokenizer = self._tokenizer\n",
    "        model = self._model\n",
    "\n",
    "        # プロンプト生成→トークン生成→GPUメモリーに転送\n",
    "        prompt = self._prompt(input_text)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # 出力取得用のストリーマー\n",
    "        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "        thread = threading.Thread(\n",
    "            target=model.generate,\n",
    "            kwargs={\n",
    "                **inputs,\n",
    "                \"streamer\": streamer,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": do_sample,\n",
    "            }\n",
    "        )\n",
    "        thread.start()\n",
    "\n",
    "        for token in streamer:\n",
    "            yield token\n",
    "\n",
    "    def print_inference_result(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        do_sample: bool = True,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> None:\n",
    "        \"\"\" 推論結果を出力\n",
    "\n",
    "        :param input_text: 入力テキスト\n",
    "        :param max_new_tokens: 生成する最大トークン数\n",
    "        :param do_sample: 推論結果をサンプリングするか？Falseなら常に確率の一番高いトークンのみを出力する（結果は決定的になる）\n",
    "        :param temperature: サンプリングの多様性を制御する温度パラメーター\n",
    "        :param top_p: nucleus samplingの確率閾値\n",
    "        \"\"\"\n",
    "        # 入力プロンプト\n",
    "        for token in self.infer(\n",
    "            input_text,\n",
    "            max_new_tokens,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_p,\n",
    "        ):\n",
    "            # トークンを1つずつ出力\n",
    "            print(token, end=\"\", flush=True)\n",
    "\n",
    "        # 出力の終端\n",
    "        print()\n",
    "\n",
    "    def _prompt(self, input_text: str) -> str:\n",
    "        \"\"\" 入力文字列から、モデルに合わせたプロンプトを生成する\n",
    "\n",
    "        :param input_text: 入力テキスト\n",
    "        :return: プロンプト\n",
    "        \"\"\"\n",
    "\n",
    "        # chat_template に対応していれば使う（Mistral, Gemmaなど）\n",
    "        if hasattr(self._tokenizer, \"chat_template\"):\n",
    "            try:\n",
    "                return self._tokenizer.apply_chat_template(\n",
    "                    [\n",
    "                        {\"role\": \"user\", \"content\": input_text},\n",
    "                    ],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "            except Exception:\n",
    "                # fallback\n",
    "                pass\n",
    "\n",
    "        # モデルに合わせたプロンプトを生成\n",
    "        model_name = self._model_name.lower()\n",
    "        if \"elyza\" in model_name:\n",
    "            return f\"[INST] {input_text} [/INST]\"\n",
    "\n",
    "        if \"rinna\" in model_name:\n",
    "            return (\n",
    "                f\"ユーザー: {input_text}\\n\"\n",
    "                \"システム: \"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            \"llama-2\" in model_name or \"llama2\" in model_name or\n",
    "            \"mistral\" in model_name or \"gemma\" in model_name\n",
    "        ):\n",
    "            return f\"<s>[INST] {input_text} [/INST]</s>\"\n",
    "\n",
    "        if \"openchat\" in model_name:\n",
    "            return f\"<|user|>\\n{input_text}\\n<|assistant|>\\n\"\n",
    "\n",
    "        if \"llama-3\" in model_name or \"llama3\" in model_name:\n",
    "            return (\n",
    "                \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "                f\"{input_text}\\n\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "            )\n",
    "\n",
    "        # デフォルト（そのまま）\n",
    "        return input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93f486-88ce-467d-82a5-21d86758f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャットアプリケーション\n",
    "# LLMクラスのインポートがないことと終了時に sys.exit() を使っていないこと以外は ./llm-chat.py と同じ\n",
    "import sys\n",
    "\n",
    "\n",
    "def main(_argv: list[str]) -> int:\n",
    "    \"\"\" メイン関数\n",
    "\n",
    "    :return: 終了ステータス\n",
    "    \"\"\"\n",
    "    # モデルの初期化\n",
    "    llm = LLM()\n",
    "\n",
    "    print(\"Now, let's talk!\")\n",
    "\n",
    "    # ひたすらチャット\n",
    "    while True:\n",
    "        try:\n",
    "            # 入力プロンプト\n",
    "            print(\"> \", end=\"\", flush=True)\n",
    "\n",
    "            input_text = input().strip()\n",
    "            if input_text in (\"exit\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            llm.print_inference_result(input_text)\n",
    "\n",
    "            # 次の入力プロンプトとの間隔を空ける\n",
    "            print()\n",
    "\n",
    "        except EOFError:\n",
    "            # EOFでも終了\n",
    "            break\n",
    "\n",
    "    print(\"See you!\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
